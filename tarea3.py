# -*- coding: utf-8 -*-
"""TAREA3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sVLTneaDlaUkYYERZ1PlnZQkN3HZ8x5F

Tarea 3 — Narrativa visual con datos
Proyecto: ¿Qué hace que un título en Netflix explote globalmente?
Framework narrativo: Setup – Insight – Implicación

Este script reproduce el análisis, las visualizaciones
y el modelo utilizados en la presentación final (PPT).

librerias
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

"""cargar base de datos"""

from google.colab import files
uploaded = files.upload()

import pandas as pd

df = pd.read_csv("netflix_titles.csv")
df.head()

import numpy as np
import pandas as pd

# df ya está cargado desde netflix_titles.csv

# Función para contar items en columnas tipo "A, B, C"
def count_items(x):
    if pd.isna(x):
        return 0
    return len([i.strip() for i in str(x).split(",") if i.strip()])

# Contar países y géneros
df["num_countries"] = df["country"].apply(count_items)
df["num_genres"] = df["listed_in"].apply(count_items)

# Etiqueta: potencial global (1) si tiene >= 2 países o >= 2 géneros
df["global_potential"] = ((df["num_countries"] >= 2) | (df["num_genres"] >= 2)).astype(int)

# Features y target
X = df[["description", "listed_in", "type"]]
y = df["global_potential"]

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

y.value_counts(), y_train.value_counts(), y_test.value_counts()

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import FunctionTransformer
from sklearn.impute import SimpleImputer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import OneHotEncoder
from sklearn.linear_model import LogisticRegression

# Convierte una única columna en un vector 1D (string)
to_1d = FunctionTransformer(lambda x: x.squeeze(), validate=False)

# Pipelines textuales
text_desc_pipe = Pipeline([
    ("impute", SimpleImputer(strategy="constant", fill_value="")),
    ("to1d", to_1d),
    ("tfidf", TfidfVectorizer(max_features=5000, ngram_range=(1, 2)))
])

text_genre_pipe = Pipeline([
    ("impute", SimpleImputer(strategy="constant", fill_value="")),
    ("to1d", to_1d),
    ("tfidf", TfidfVectorizer(max_features=2000))
])

# Pipeline categórica
type_pipe = Pipeline([
    ("impute", SimpleImputer(strategy="most_frequent")),
    ("to1d", FunctionTransformer(lambda x: x.astype(str).squeeze(), validate=False)),
    ("reshape", FunctionTransformer(lambda x: x.reshape(-1,1), validate=False)),
    ("ohe", OneHotEncoder(handle_unknown="ignore"))
])

# ColumnTransformer CORRECTO
preprocessor = ColumnTransformer(
    transformers=[
        ("desc", text_desc_pipe, ["description"]),
        ("genre", text_genre_pipe, ["listed_in"]),
        ("type", type_pipe, ["type"])
    ]
)

# Modelo final
model = Pipeline([
    ("preprocess", preprocessor),
    ("clf", LogisticRegression(max_iter=500))
])

model.fit(X_train, y_train)

from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report, roc_curve
)

y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]

acc = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred)
rec = recall_score(y_test, y_pred)
f1  = f1_score(y_test, y_pred)
auc_value = roc_auc_score(y_test, y_prob)

acc, prec, rec, f1, auc_value

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, roc_curve, auc

# ============================
# 1. Calcular métricas
# ============================

cm = confusion_matrix(y_test, y_pred, normalize='true')
fpr, tpr, _ = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

metric_names = ["Accuracy", "Precision", "Recall", "F1", "AUC"]
metric_values = [acc, prec, rec, f1, roc_auc]

# ============================
# 2. Crear figura con 3 subplots
# ============================

fig, axes = plt.subplots(1, 3, figsize=(22, 6))

# ============================
# Subplot 1: Matriz de Confusión
# ============================
sns.heatmap(cm, annot=True, fmt=".2f", cmap="Blues",
            xticklabels=["No Global", "Global"],
            yticklabels=["No Global", "Global"],
            cbar=False, ax=axes[0], linewidths=.5, linecolor='gray')
axes[0].set_title("Matriz de Confusión (Normalizada)", fontsize=14)
axes[0].set_xlabel("Predicción")
axes[0].set_ylabel("Valor Real")

# ============================
# Subplot 2: Curva ROC
# ============================
axes[1].plot(fpr, tpr, color="#0A2342", linewidth=2, label=f"AUC = {roc_auc:.3f}")
axes[1].plot([0,1], [0,1], "--", color="gray", linewidth=1)
axes[1].set_title("Curva ROC", fontsize=14)
axes[1].set_xlabel("False Positive Rate")
axes[1].set_ylabel("True Positive Rate")
axes[1].legend(loc="lower right")
axes[1].grid(alpha=0.3)

# ============================
# Subplot 3: Métricas del Modelo
# ============================
bars = axes[2].bar(metric_names, metric_values, color="#3A86FF", alpha=0.85)

for bar, val in zip(bars, metric_values):
    axes[2].text(bar.get_x() + bar.get_width()/2, val + 0.02,
                 f"{val:.2f}", ha="center", fontsize=12)

axes[2].set_ylim(0, 1.15)
axes[2].set_title("Resumen de Métricas", fontsize=14)
axes[2].set_ylabel("Valor")
axes[2].grid(axis="y", alpha=0.25)

plt.tight_layout()
plt.show()

import seaborn as sns

# --------------------------------------------------
# EDA 3: Preparación de duración por tipo de contenido
# --------------------------------------------------

df_violin = df[['type', 'duration']].dropna().copy()

# Extraer valor numérico desde el texto de duración
df_violin['duration_numeric'] = (
    df_violin['duration']
    .str.extract('(\d+)')
    .astype(int)
)

# Filtrar valores extremos solo para visualización
df_violin = df_violin[
    ((df_violin['type'] == 'Movie') & (df_violin['duration_numeric'] <= 300)) |
    ((df_violin['type'] == 'TV Show') & (df_violin['duration_numeric'] <= 20))
]

# --------------------------------------------------
# Preparación: duración normalizada
# --------------------------------------------------

df_duration = df[['type', 'duration']].dropna().copy()

# Extraer número desde el texto de duración
df_duration['duration_value'] = (
    df_duration['duration']
    .str.extract('(\d+)')
    .astype(int)
)

# Normalización:
# - Movie: minutos reales
# - TV Show: temporadas * 60 minutos (aprox.)
def normalize_duration(row):
    if row['type'] == 'Movie':
        return row['duration_value']
    else:
        return row['duration_value'] * 60

df_duration['duration_normalized'] = df_duration.apply(
    normalize_duration,
    axis=1
)

# Filtrar extremos solo para visualización
df_duration = df_duration[
    ((df_duration['type'] == 'Movie') & (df_duration['duration_normalized'] <= 300)) |
    ((df_duration['type'] == 'TV Show') & (df_duration['duration_normalized'] <= 700))
]

# --------------------------------------------------
# EDA: Comparación de duración normalizada
# --------------------------------------------------

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))

sns.boxplot(
    data=df_duration,
    x='type',
    y='duration_normalized',
    palette=['#4c72b0', '#9b59b6'],
    showfliers=True
)

plt.title('Comparación de duración normalizada entre Películas y Series')
plt.xlabel('Tipo de contenido')
plt.ylabel('Duración normalizada (minutos equivalentes)')

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# usar la probabilidad que YA existe
scores_no_global = y_prob[y_test == 0]
scores_global = y_prob[y_test == 1]

plt.figure(figsize=(8, 5))

plt.hist(scores_no_global, bins=30, alpha=0.6, density=True, label='No Global')
plt.hist(scores_global, bins=30, alpha=0.6, density=True, label='Global')

plt.xlabel('Score de potencial global')
plt.ylabel('Densidad')
plt.title('Síntesis: separación de títulos según score predictivo')
plt.legend()
plt.tight_layout()
plt.show()

# Número de features por bloque
n_desc = (
    model.named_steps['preprocess']
    .named_transformers_['desc']
    .named_steps['tfidf']
    .get_feature_names_out()
    .shape[0]
)

n_genre = (
    model.named_steps['preprocess']
    .named_transformers_['genre']
    .named_steps['tfidf']
    .get_feature_names_out()
    .shape[0]
)

n_type = (
    model.named_steps['preprocess']
    .named_transformers_['type']
    .named_steps['ohe']
    .get_feature_names_out()
    .shape[0]
)

# número de features de description
n_desc = 5000

# coeficientes SOLO del bloque description
coefficients_desc = model.named_steps['clf'].coef_[0][:n_desc]

# nombres de palabras
feature_names_desc = (
    model.named_steps['preprocess']
    .named_transformers_['desc']
    .named_steps['tfidf']
    .get_feature_names_out()
)

coef_df = pd.DataFrame({
    'word': feature_names_desc,
    'coef': coefficients_desc
})

top_pos = coef_df.sort_values('coef', ascending=False).head(12)
top_neg = coef_df.sort_values('coef', ascending=True).head(12)

coef_plot = pd.concat([top_neg, top_pos])

plt.figure(figsize=(10, 6))

colors = ['#c0392b' if c < 0 else '#2c3e50' for c in coef_plot['coef']]

plt.barh(coef_plot['word'], coef_plot['coef'], color=colors)
plt.axvline(0, color='black', linewidth=1)

plt.title('Señales textuales predictoras de potencial global')
plt.xlabel('Coeficiente (TF-IDF + Logistic Regression)')
plt.ylabel('Palabra clave')

plt.tight_layout()
plt.show()